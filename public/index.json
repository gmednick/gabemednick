[{"authors":["gabe"],"categories":null,"content":"My journey into the physical and biological sciences started with a desire to study osteopathic medicine. In the process of completing a biochemistry and molecular biology degree, my interest in the structure and function of the human body grew into a fascination with the invisible structure and inner workings of the cell. I developed a deep interest in both physical chemistry and biochemistry, and my curiosity resulted in a PhD focussed on sensory transduction pathways and light sensing mechanisms in bacteria.\nAfter finishing my PhD, I developed and implemented innovative teaching practices in chemistry and biology at the university level. More recently (2018-2020), I worked as a senior scientist for a small startup with an emphasis on DNA and RNA synthesis. I love hands-on research but my current professional passion is centered on data science and bioinformatics. My mission is to facilitate data informed choices that provide insight, drive innovation and optimize decision making.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"9d5326a2f1cf09d7e5a9f14131238744","permalink":"/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"My journey into the physical and biological sciences started with a desire to study osteopathic medicine. In the process of completing a biochemistry and molecular biology degree, my interest in the structure and function of the human body grew into a fascination with the invisible structure and inner workings of the cell.","tags":null,"title":"","type":"authors"},{"authors":[],"categories":[],"content":" Iris data The iris dataset is a classic, so much so that it‚Äôs included in the datasets package that comes with every base installation of R. You can use data() to see a list of all available datasets. Datasets that are associated with packages can be found in a similar way, e.g., data(package = 'dplyr').\nLet‚Äôs take a look at the data.\n# load the iris data set and clean the column names with janitor::clean_names() iris_df\u0026lt;- iris %\u0026gt;% clean_names() iris_df %\u0026gt;% head() ## sepal_length sepal_width petal_length petal_width species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa iris_df %\u0026gt;% count(species) ## species n ## 1 setosa 50 ## 2 versicolor 50 ## 3 virginica 50 # equal number of each species, 150 total iris_df %\u0026gt;% str() ## \u0026#39;data.frame\u0026#39;: 150 obs. of 5 variables: ## $ sepal_length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ sepal_width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ petal_length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ petal_width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ species : Factor w/ 3 levels \u0026quot;setosa\u0026quot;,\u0026quot;versicolor\u0026quot;,..: 1 1 1 1 1 1 1 1 1 1 ... This dataset is clean, but with only 150 observations, we must question whether it‚Äôs too small to train a reliable model. We will generate bootstrap resamples to make the most of what we have.\nWe see that we have four features (sepal length and width, and petal length and width) and there are three unique species..\nIt makes sense to create a model that predicts the species of iris based on the flower‚Äôs measurements.\n Visualize relationships Before we do any kind of machine learning, it‚Äôs helpful to visualize the data and develop a better understanding of the relationships in our data. This will also help us to have an intuitive sense about the potential predictive power of the data.\nsepal \u0026lt;- iris_df %\u0026gt;% ggplot(aes(sepal_length, sepal_width, color = species)) + geom_point(size = 1) + facet_wrap(~species) + labs(x = \u0026#39;sepal length\u0026#39;, y = \u0026#39;sepal width\u0026#39;) + theme(legend.position = \u0026#39;none\u0026#39;) petal \u0026lt;- iris_df %\u0026gt;% ggplot(aes(petal_length, petal_width, color = species)) + geom_point(size =1) + facet_wrap(~species) + labs(x = \u0026#39;petal length\u0026#39;, y = \u0026#39;petal width\u0026#39;) + theme(legend.position = \u0026#39;none\u0026#39;) (petal/sepal) # patchwork allows us to arrange plots side-by-side or stacked but there\u0026#39;s a better way..  Tidy the dataset Let‚Äôs change the shape of our data by combining the four iris features into a single column (named metric) and the associated values will populate a new column (named value). This transformation into a longer dataset can be achieved with the function pivot_longer(). As we shall see, the tidy format lends itself to data visualization and analysis with the tidyverse.\niris_df_long \u0026lt;- iris_df %\u0026gt;% pivot_longer(cols = sepal_length:petal_width, names_to = \u0026#39;metric\u0026#39;, values_to =\u0026#39;value\u0026#39;) # A boxplot is a great way to see the median values of our features by species. iris_df_long %\u0026gt;% ggplot(aes(metric, value, color = species)) + geom_boxplot()  # A nice alternative is to facet by the metric to convey the same information but with added clarity. iris_df_long %\u0026gt;% ggplot(aes(species, value, color = species)) + geom_boxplot(alpha = 0.3) + facet_wrap(~ metric, scales = \u0026quot;free_y\u0026quot;) # The same information can be displayed by comparing the distributions in histogram. iris_df_long %\u0026gt;% ggplot(aes(value, fill = species)) + geom_histogram(bins = 20, alpha = 0.7) + facet_wrap(~ metric, scales = \u0026quot;free_x\u0026quot;) # geom_denisty is a nice alternative to geom_histogram. iris_df_long %\u0026gt;% ggplot(aes(value, fill = species)) + geom_density(alpha = .5) + facet_wrap(~ metric, scales = \u0026quot;free\u0026quot;)  Get modelling Before we get to modeling, we need to split the data. Since our dataset is small, we are going to take the training set and make bootstrap resamples. By default, initial split provides a 75:25 split for our train and test sets respectively. The function bootstraps will take the training data, further split it into a training and test set, then resample and repeat 25 times. This ‚Äòresampling‚Äô provides a more robust dataset to train our model with.\nset.seed(123) tidy_split \u0026lt;- initial_split(iris_df) tidy_split ## \u0026lt;Analysis/Assess/Total\u0026gt; ## \u0026lt;113/37/150\u0026gt; iris_train \u0026lt;- training(tidy_split) iris_test \u0026lt;- testing(tidy_split) iris_boots \u0026lt;- bootstraps(iris_train) iris_boots ## # Bootstrap sampling ## # A tibble: 25 x 2 ## splits id ## \u0026lt;list\u0026gt; \u0026lt;chr\u0026gt; ## 1 \u0026lt;split [113/39]\u0026gt; Bootstrap01 ## 2 \u0026lt;split [113/40]\u0026gt; Bootstrap02 ## 3 \u0026lt;split [113/44]\u0026gt; Bootstrap03 ## 4 \u0026lt;split [113/41]\u0026gt; Bootstrap04 ## 5 \u0026lt;split [113/39]\u0026gt; Bootstrap05 ## 6 \u0026lt;split [113/40]\u0026gt; Bootstrap06 ## 7 \u0026lt;split [113/42]\u0026gt; Bootstrap07 ## 8 \u0026lt;split [113/35]\u0026gt; Bootstrap08 ## 9 \u0026lt;split [113/42]\u0026gt; Bootstrap09 ## 10 \u0026lt;split [113/41]\u0026gt; Bootstrap10 ## # ‚Ä¶ with 15 more rows  Recipes Recipes is a powerful tool with functions for a wide range of feature engineering tasks designed to prepare data for modeling. Typing recipes:: into the Rstudio console is a great way to browse the available functions in the package.\nLet‚Äôs create a recipe to demonstrate how easy it is to apply feature engineering. There is really no need for feature engineering with this dataset, so we won‚Äôt actually use this recipe in the final workflow.\niris_rec \u0026lt;- recipe(species ~., data = iris_train) %\u0026gt;% step_pca(all_predictors()) %\u0026gt;% step_normalize(all_predictors()) prep \u0026lt;- prep(iris_rec) kable(head(iris_juice \u0026lt;- juice(prep)))   species PC1 PC2 PC3 PC4    setosa 1.1621621 1.384382 -0.0270717 0.0422303  setosa 1.3983644 1.159657 0.7767696 0.6840123  setosa 1.4836145 1.244589 0.0010748 0.1379599  setosa 1.4923559 1.089539 0.0331285 -0.4892659  setosa 1.1873971 1.402225 -0.4220083 -0.3874109  setosa 0.7736181 1.394953 -0.8518454 -0.1532614     Creating models with Parsnip Let‚Äôs set up two different models: first, a generalized linear model or glmnet. In this step we will create the model, workflow and fit the bootstraps. Let‚Äôs take a look at the output from each step.\n# set seed set.seed(1234) # generate the glmnet model with parsnip glmnet_mod \u0026lt;- multinom_reg(penalty = 0) %\u0026gt;% set_engine(\u0026quot;glmnet\u0026quot;) %\u0026gt;% set_mode(\u0026quot;classification\u0026quot;) glmnet_mod ## Multinomial Regression Model Specification (classification) ## ## Main Arguments: ## penalty = 0 ## ## Computational engine: glmnet # create a workflow glmnet_wf \u0026lt;- workflow() %\u0026gt;% add_formula(species ~ .) glmnet_wf ## ‚ïê‚ïê Workflow ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê ## Preprocessor: Formula ## Model: None ## ## ‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ## species ~ . # add the model to the workflow and use iris_boots to fit our model 25 times glmnet_results \u0026lt;- glmnet_wf %\u0026gt;% add_model(glmnet_mod) %\u0026gt;% fit_resamples( resamples = iris_boots, control = control_resamples(extract = extract_model, save_pred = TRUE) ) glmnet_results ## # Resampling results ## # Bootstrap sampling ## # A tibble: 25 x 6 ## splits id .metrics .notes .extracts .predictions ## \u0026lt;list\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; ## 1 \u0026lt;split [113‚Ä¶ Bootstra‚Ä¶ \u0026lt;tibble [2 √ó‚Ä¶ \u0026lt;tibble [0 ‚Ä¶ \u0026lt;tibble [1 ‚Ä¶ \u0026lt;tibble [39 √ó‚Ä¶ ## 2 \u0026lt;split [113‚Ä¶ Bootstra‚Ä¶ \u0026lt;tibble [2 √ó‚Ä¶ \u0026lt;tibble [0 ‚Ä¶ \u0026lt;tibble [1 ‚Ä¶ \u0026lt;tibble [40 √ó‚Ä¶ ## 3 \u0026lt;split [113‚Ä¶ Bootstra‚Ä¶ \u0026lt;tibble [2 √ó‚Ä¶ \u0026lt;tibble [0 ‚Ä¶ \u0026lt;tibble [1 ‚Ä¶ \u0026lt;tibble [44 √ó‚Ä¶ ## 4 \u0026lt;split [113‚Ä¶ Bootstra‚Ä¶ \u0026lt;tibble [2 √ó‚Ä¶ \u0026lt;tibble [0 ‚Ä¶ \u0026lt;tibble [1 ‚Ä¶ \u0026lt;tibble [41 √ó‚Ä¶ ## 5 \u0026lt;split [113‚Ä¶ Bootstra‚Ä¶ \u0026lt;tibble [2 √ó‚Ä¶ \u0026lt;tibble [0 ‚Ä¶ \u0026lt;tibble [1 ‚Ä¶ \u0026lt;tibble [39 √ó‚Ä¶ ## 6 \u0026lt;split [113‚Ä¶ Bootstra‚Ä¶ \u0026lt;tibble [2 √ó‚Ä¶ \u0026lt;tibble [0 ‚Ä¶ \u0026lt;tibble [1 ‚Ä¶ \u0026lt;tibble [40 √ó‚Ä¶ ## 7 \u0026lt;split [113‚Ä¶ Bootstra‚Ä¶ \u0026lt;tibble [2 √ó‚Ä¶ \u0026lt;tibble [0 ‚Ä¶ \u0026lt;tibble [1 ‚Ä¶ \u0026lt;tibble [42 √ó‚Ä¶ ## 8 \u0026lt;split [113‚Ä¶ Bootstra‚Ä¶ \u0026lt;tibble [2 √ó‚Ä¶ \u0026lt;tibble [0 ‚Ä¶ \u0026lt;tibble [1 ‚Ä¶ \u0026lt;tibble [35 √ó‚Ä¶ ## 9 \u0026lt;split [113‚Ä¶ Bootstra‚Ä¶ \u0026lt;tibble [2 √ó‚Ä¶ \u0026lt;tibble [0 ‚Ä¶ \u0026lt;tibble [1 ‚Ä¶ \u0026lt;tibble [42 √ó‚Ä¶ ## 10 \u0026lt;split [113‚Ä¶ Bootstra‚Ä¶ \u0026lt;tibble [2 √ó‚Ä¶ \u0026lt;tibble [0 ‚Ä¶ \u0026lt;tibble [1 ‚Ä¶ \u0026lt;tibble [41 √ó‚Ä¶ ## # ‚Ä¶ with 15 more rows # look at the model metrics collect_metrics(glmnet_results) ## # A tibble: 2 x 5 ## .metric .estimator mean n std_err ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 accuracy multiclass 0.944 25 0.00642 ## 2 roc_auc hand_till 0.987 25 0.00234 Now for a random forest model. We only need to tweak a few things and walah!\nset.seed(1234) rf_mod \u0026lt;- rand_forest() %\u0026gt;% set_engine(\u0026quot;ranger\u0026quot;) %\u0026gt;% set_mode(\u0026quot;classification\u0026quot;) # We set up a workflow and add the parts of our model together like legos rf_wf \u0026lt;- workflow() %\u0026gt;% add_formula(species ~ .) # Here we fit our 25 resampled datasets rf_results \u0026lt;- rf_wf %\u0026gt;% add_model(rf_mod) %\u0026gt;% fit_resamples( resamples = iris_boots, control = control_resamples(save_pred = TRUE) ) collect_metrics(rf_results) ## # A tibble: 2 x 5 ## .metric .estimator mean n std_err ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 accuracy multiclass 0.945 25 0.00764 ## 2 roc_auc hand_till 0.993 25 0.00126 Here‚Äôs a look at the confusion matrix summaries for both models. The confusion matrix let‚Äôs us see the correct and incorrect predictions of our models in a single table.\nglmnet_results %\u0026gt;% conf_mat_resampled()  ## # A tibble: 9 x 3 ## Prediction Truth Freq ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 setosa setosa 13.3 ## 2 setosa versicolor 0 ## 3 setosa virginica 0 ## 4 versicolor setosa 0 ## 5 versicolor versicolor 12.2 ## 6 versicolor virginica 1 ## 7 virginica setosa 0 ## 8 virginica versicolor 1.24 ## 9 virginica virginica 12.4 rf_results %\u0026gt;% conf_mat_resampled()  ## # A tibble: 9 x 3 ## Prediction Truth Freq ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 setosa setosa 13.3 ## 2 setosa versicolor 0 ## 3 setosa virginica 0 ## 4 versicolor setosa 0 ## 5 versicolor versicolor 12.3 ## 6 versicolor virginica 1.04 ## 7 virginica setosa 0 ## 8 virginica versicolor 1.16 ## 9 virginica virginica 12.4 The ROC curve helps us visually interpret our model performance at every threshold.\nglmnet_results %\u0026gt;% collect_predictions() %\u0026gt;% group_by(id) %\u0026gt;% roc_curve(species, .pred_setosa:.pred_virginica) %\u0026gt;% autoplot() rf_results %\u0026gt;% collect_predictions() %\u0026gt;% group_by(id) %\u0026gt;% roc_curve(species, .pred_setosa:.pred_virginica) %\u0026gt;% autoplot() + theme(legend.position = \u0026#39;none\u0026#39;)  Final fit This is it! By using the last_fit(tidy_split), we are able to train our model on the training set and test the model on the testing set in one fell swoop! Note, this is the only time we use the test set.\nfinal_glmnet \u0026lt;- glmnet_wf %\u0026gt;% add_model(glmnet_mod) %\u0026gt;% last_fit(tidy_split) final_glmnet ## # Resampling results ## # Monte Carlo cross-validation (0.75/0.25) with 1 resamples ## # A tibble: 1 x 6 ## splits id .metrics .notes .predictions .workflow ## \u0026lt;list\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; ## 1 \u0026lt;split [113‚Ä¶ train/test ‚Ä¶ \u0026lt;tibble [2 √ó‚Ä¶ \u0026lt;tibble [0 ‚Ä¶ \u0026lt;tibble [37 √ó ‚Ä¶ \u0026lt;workflo‚Ä¶ final_rf \u0026lt;- rf_wf %\u0026gt;% add_model(rf_mod) %\u0026gt;% last_fit(tidy_split) final_rf ## # Resampling results ## # Monte Carlo cross-validation (0.75/0.25) with 1 resamples ## # A tibble: 1 x 6 ## splits id .metrics .notes .predictions .workflow ## \u0026lt;list\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; ## 1 \u0026lt;split [113‚Ä¶ train/test ‚Ä¶ \u0026lt;tibble [2 √ó‚Ä¶ \u0026lt;tibble [0 ‚Ä¶ \u0026lt;tibble [37 √ó ‚Ä¶ \u0026lt;workflo‚Ä¶  Confusion Matrices Finally, let‚Äôs generate a multiclass confusion matrix with the results from our test data. The confusion matrix provides a count of each outcome for all possible outcomes. The columns contain the true values and predictions are set across rows. This confusion matrix might look confusing because all predictions are correct.\ncollect_metrics(final_glmnet) ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 accuracy multiclass 1 ## 2 roc_auc hand_till 1 collect_predictions(final_glmnet) %\u0026gt;% conf_mat(species, .pred_class) %\u0026gt;% autoplot(type = \u0026#39;heatmap\u0026#39;)  collect_metrics(final_rf) ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 accuracy multiclass 0.973 ## 2 roc_auc hand_till 1 collect_predictions(final_rf) %\u0026gt;% conf_mat(species, .pred_class) %\u0026gt;% autoplot(type = \u0026#39;heatmap\u0026#39;)  Final thoughts It‚Äôs always good to ask the question, ‚ÄòDo my results make sense‚Äô? Both models show near perfect predictive power but are they really that good? From our visual analysis, we can confidently say yes, there is a clear distinction between species when comparing the explanatory features. Now that we have a model, we have the power to predict iris type on new data üòÑ.\nI would like to thank my mentors, the three noble Jedi Knights of the tidyverse and tidymodels: Julia Silge, David Robinson and Andrew Couch.\n ","date":1603238400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603318262,"objectID":"517bb6adcf89c779300b0648b798dbfb","permalink":"/post/iris-classification/","publishdate":"2020-10-21T00:00:00Z","relpermalink":"/post/iris-classification/","section":"post","summary":"Flower classification using Tidymodels","tags":["machine learning","tidymodels"],"title":"Iris Classification","type":"post"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"6087c0ef875554f4409ac52928d79279","permalink":"/projects/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/projects/","section":"","summary":"See some of the projects I have worked on","tags":null,"title":"Projects","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"/about/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/about/","section":"","summary":"A little more about me and how to get in touch","tags":null,"title":"Resume","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"65de3680a280f6bf29dc34fe1adad5a6","permalink":"/talks/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/talks/","section":"","summary":"Upcoming and recent talks / workshops","tags":null,"title":"Talks \u0026 Workshops","type":"widget_page"},{"authors":null,"categories":null,"content":"My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.\n   ","date":1530140400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530140400,"objectID":"53e892b8b41cc4caece1cfd5ef21d6e7","permalink":"/license/","publishdate":"2018-06-28T00:00:00+01:00","relpermalink":"/license/","section":"","summary":"My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.\n   ","tags":null,"title":"LICENSE: CC-BY-SA","type":"page"}]