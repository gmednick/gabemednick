---
title: "Housing Price Predictions (part 2)"
author: "Gabe Mednick"
date: '2021-06-15'
output:
  html_document:
    df_print: paged
categories: []
tags: []
subtitle: ''
summary: Comparing regression models using the tidymodels framework for machine learning
authors: []
lastmod: ''
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
slug: AmesHousing2
---

In this post, we will use the Ames housing data to compare several models. The models will be trained to predict sale price and performance will be measured by the root mean squared error (rmse).   

We will compare a simple linear regression (lm) with glmnet (lasso and elastic-net regularized generalized linear model), decision trees, random forest and xgboost models. Where appropriate, we will also try to improve model performance with hyperparameter tuning.  Let's get to it!

```{r setup, include=FALSE} 
knitr::opts_chunk$set(echo=TRUE, message=FALSE, warning=FALSE)
library(tidyverse)
library(tidymodels)
library(AmesHousing)
theme_set(theme_light())
doParallel::registerDoParallel(cores = 4) #parallel processing: allows multiple cores to work while training the resampled data.
```


```{r}
ames_df <- make_ames() %>% 
  janitor::clean_names()  %>% # extracting the data from the AmesHousing package and converting all column names to lower, snake_case
  mutate(sale_price = log10(sale_price))
```


## Splitting and creating k-fold cross-validation data

```{r}
library(rsample)
set.seed(518)

ames_split <- initial_split(ames_df, prop = 0.8, strata = "sale_price") #strata argument will insure that the prediction variable is equally represented in the training and test sets

ames_train <- training(ames_split)
ames_test <- testing(ames_split)

folds <- vfold_cv(ames_train, v = 10, strata = "sale_price")
```

## Variable importance

Before we delve into building models, let's consider which features have the most explanatory power based on the random forest model.

```{r}

#use vip (variable importance package to predict the importance of our expanatory variables)
rand_forest(mode = "regression") %>%
  set_engine("ranger", importance = "impurity") %>%
  fit(sale_price ~ ., 
      data = ames_train) %>%
  vip::vip()

ames_train$overall_cond %>% levels() 

rand_forest(mode = "regression") %>%
  set_engine("ranger", importance = "impurity") %>%
  fit(sale_price ~ neighborhood + gr_liv_area + year_built + bldg_type, 
      data = ames_train) %>%
  vip::vip()

# Let's visualize the relationship between sale price and gr_liv_area. 
ggplot(ames_train, aes(x = gr_liv_area, y = sale_price)) + 
  geom_point(alpha = .2) + 
  facet_wrap(~ bldg_type) + 
  geom_smooth(method = lm, formula = y ~ x, se = FALSE, col = "red") + 
  scale_x_log10() + 
  scale_y_log10() + 
  labs(x = "Gross Living Area", y = "Sale Price (USD)")
```

## Data preprocessing and formula specification

The feature engineering steps in this post were adapted from Julia Silge and Max Khun's [tidymodels with R](https://www.tmwr.org/) textbook. 

The formula specification has the form `prediction_variable ~ predictor_variables`. In this model, the predictor variables include neighborhood, living area, year built and building type.

```{r}
ames_rec <- 
  recipe(sale_price ~ overall_cond + gr_liv_area + year_built + bldg_type,
         data = ames_train) %>%
  step_log(gr_liv_area, base = 10) %>% #log transform gr_liv_area
  step_corr(all_numeric(), - all_outcomes()) %>% #remove variables with large correlations
  step_nzv(all_predictors()) %>% #remove variables that are highly sparse and unbalanced
  #step_other(neighborhood, threshold = 0.01) %>% #pool occurring neighborhoods into an "other" category
  step_dummy(all_nominal_predictors()) %>% #converts neighborhoods and building type into a numeric binary model
  step_interact( ~ gr_liv_area:starts_with("bldg_type_")) #will create new columns based on interactions between the specified features

ames_rec %>%  prep() %>% juice() #view the transformed data
```

## Model evaluation

We expect our model to be better than random chance but how do we know? We can generate the value of a dummy (random chance) model by calculating the standard deviation of sale price.

```{r}
sd(ames_train$sale_price)
# Random chance value: 0.1787969
```

## Model 1: linear model

```{r}
set.seed(212)

ames_lm <- linear_reg() %>% 
  set_engine("lm")

ames_wkflow <- workflow() %>% 
  add_model(ames_lm) %>% 
  add_recipe(ames_rec)

ames_model_train <- ames_wkflow %>% 
  fit_resamples(resamples = folds,
                metrics = metric_set(rmse))

# best_lm <- select_best(ames_model_train)
# 
# final_wkflow <- finalize_workflow(ames_wkflow, best_lm)
# 
# final_lm_res <- last_fit(final_wkflow, ames_split)
# 
# final_lm_res %>%  collect_metrics() # rmse: 0.07682275


ames_model_train %>% 
  collect_metrics(summarize = F) %>% 
  arrange(.estimate) %>% 
  filter(.metric == "rmse") %>% 
  select(rmse = .estimate) %>% 
  head(1)

# mean rmse for linear model: 0.07974068 
```

## Model 2: Glmnet 

```{r}
set.seed(212)

ames_glmnet <- linear_reg(penalty = tune(), 
                          mixture = tune()) %>% 
  set_engine("glmnet") %>% 
  set_mode("regression")

ames_glmnet_wkflow <- workflow() %>% 
  add_model(ames_glmnet) %>% 
  add_recipe(ames_rec)

glmnet_grid <- grid_random(parameters(ames_glmnet),
                           size = 7)

glmnet_tunegrid <- ames_glmnet_wkflow %>%
  tune_grid(resamples = folds,
            grid = glmnet_grid,
            metrics = metric_set(rmse),
            control = control_grid(save_pred = TRUE))

glmnet_tunegrid %>% collect_metrics()

autoplot(glmnet_tunegrid)

best_rmse <- select_best(glmnet_tunegrid)

final_glmnet <- finalize_workflow(ames_glmnet_wkflow, best_rmse)

final_glmnet %>% 
  fit(data = ames_train) %>% 
  pull_workflow_fit() %>% 
  vip::vip(geom = "point")

final_glmnet_res <- last_fit(final_glmnet, ames_split)

final_glmnet_res %>% 
  collect_metrics() # rmse: .0769
```

## Model 3: Decision Tree

```{r}
ames_decision_tree <- decision_tree() %>% 
  set_engine("rpart") %>% 
  set_mode("regression")

decision_tree_wkflow <- workflow() %>% 
  add_model(ames_decision_tree) %>% 
  add_recipe(ames_rec)

ames_decision_train <- decision_tree_wkflow %>% 
  fit_resamples(resamples = folds,
                metrics = metric_set(rmse))

show_best(ames_decision_train)

ames_decision_train %>% 
  collect_metrics()

best_dec_tree <- select_best(ames_decision_train)

final_dec_tree <- finalize_workflow(decision_tree_wkflow, best_dec_tree)

final_dec_results <- last_fit(final_dec_tree, ames_split)

final_dec_results %>% collect_metrics() # rmse: .09885
# mean rmse = 0.1064535
```

## Model 4: Random Forest

```{r}
ames_rand_forest <- rand_forest() %>% 
  set_engine("ranger") %>% 
  set_mode("regression")

rf_wkflow <- workflow() %>% 
  add_model(ames_rand_forest) %>% 
  add_recipe(ames_rec)

ames_rf_train <- rf_wkflow %>% 
  fit_resamples(resamples = folds,
                metrics = metric_set(rmse))

show_best(ames_rf_train) # rmse = .0825

ames_rf_train %>% 
  collect_metrics()

# mean rmse = .0823

```

## Random Forest with tuning

```{r}
set.seed(222)

ames_rand_forest_tune <- rand_forest(
  trees = 500,
  mtry = tune(),
  min_n = tune()) %>% 
  set_engine("ranger") %>% 
  set_mode("regression")

rf_wkflow <- workflow() %>% 
  add_model(ames_rand_forest_tune) %>% 
  add_recipe(ames_rec)

# hypercube_grid <- grid_latin_hypercube(
#   min_n(),
#   finalize(mtry(), ames_train)
# )


rf_tunegrid <- rf_wkflow %>%
  tune_grid(resamples = folds,
            #grid = hypercube_grid,
            metrics = metric_set(rmse),
            control = control_grid(save_pred = TRUE))


rf_tunegrid %>% 
  collect_metrics() 

rf_tunegrid %>% 
  collect_predictions()


#mean rmse: 0.07816951
```

## Model 5: Xgboost

```{r}

ames_xgboost <- boost_tree() %>% 
  set_engine("xgboost") %>% 
  set_mode("regression")

train_xgboost <- workflow() %>% 
  add_model(ames_xgboost) %>% 
  add_recipe(ames_rec) %>% 
  fit_resamples(resamples = folds,
                metrics = metric_set(rmse))

train_xgboost %>% 
  collect_metrics()

# rmse: 0.08357835
```


## Xgboost with tuning

```{r}
set.seed(123)

 # Create the specification with placeholders
boost_spec <- boost_tree(
    trees = 500,
    learn_rate = tune(),
    tree_depth = tune(),
    sample_size = tune()) %>%
  set_mode("regression") %>%
  set_engine("xgboost")

boost_wkflow <- workflow() %>% 
  add_recipe(ames_rec) %>% 
  add_model(boost_spec)

# boost_grid <- grid_random(parameters(boost_spec),
#                           size = 10)

ames_tune <- boost_wkflow %>%  
  tune_grid(resamples = folds,
            #grid = boost_grid,
            metrics = metric_set(rmse))

ames_tune %>% 
  collect_metrics()

show_best(ames_tune)

autoplot(ames_tune)

boost_tune_result <- show_best(ames_tune) %>%
  select(.metric, mean) %>% 
  arrange(mean) %>% 
  filter(row_number() == 1)

```


## Results

In this analysis, we compared various regression models and used the tune package to optimize hyperparameter values. The `collect_metrics()` function allows us to see the possible hyperparameter combinations and the resulting rmse. Let's compare the rmse values to rank model performance.

```{r}
library(kableExtra)

model <- c('linear', 'glmnet', 'decision tree', 'random forest', 'random forest tuned', 'xgboost', 'xgboost tuned')

# Function to extract best  rmse from each model
extract_rmse <- function(tbl) { 
  tbl %>% 
  collect_metrics(summarize = F) %>% 
  arrange(.estimate) %>% 
  dplyr::filter(.metric == "rmse") %>% 
  dplyr::select(rmse = .estimate) %>% 
  head(1)
}

results = list(ames_model_train, glmnet_tunegrid, ames_decision_train, ames_rf_train, rf_tunegrid, train_xgboost, ames_tune)

model_results <- map_df(results, extract_rmse) %>%
  mutate(Model = model) %>% 
  dplyr::select(Model, rmse) %>% 
  arrange(rmse)

kable(model_results, caption = "Summary of model performance")
```










