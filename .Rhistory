by_place_answer <- pizza_jared %>%
mutate(time = as.POSIXct(time, origin = "1970-01-01"),
date = as.Date(time),
answer = fct_relevel(answer, answer_orders)) %>%
group_by(place, answer) %>%
summarize(votes = sum(votes)) %>%
mutate(total = sum(votes),
percent = votes / total,
answer_integer = as.integer(answer),
average = sum(answer_integer * percent)) %>%
ungroup()
by_place <- by_place_answer %>%
distinct(place, total, average)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
food_consumption <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-18/food_consumption.csv')
#Viewing the basic summary of the dataset
#Many countries and 11 food categories
food_consumption %>%
summary()
#Distributions of CO2 and Consumption are not as proportional as I would imagine
options(scipen = 999)
food_consumption %>%
gather(key = "feature", value = "value", -country, -food_category) %>%
ggplot(aes(x = feature, y = value, color = feature)) +
geom_boxplot() +
facet_wrap(~feature, scales = "fixed") +
scale_y_log10()
food_consumption %>%
gather(key = "feature", value = "value", -country, -food_category) %>%
ggplot(aes(x = feature, y = value, color = feature)) +
geom_boxplot() +
geom_jitter(alpha = .1) +
facet_wrap(~feature, scales = "fixed") +
scale_y_log10()
food_consumption %>%
filter(consumption != 0) %>%
mutate(co2perfood = co2_emmission/consumption) %>%
group_by(food_category) %>%
summarise(avg_c02perfood = mean(co2perfood)) %>%
ggplot(aes(x = reorder(food_category, avg_c02perfood), y = avg_c02perfood, fill = food_category)) +
geom_col() +
coord_flip() +
theme(legend.position = "none")
food_consumption %>%
select(-country) %>%
gather(key = "feature", value = "value", -food_category) %>%
ggplot(aes(x = food_category, y = value, color = food_category)) +
geom_boxplot() +
scale_y_log10() +
facet_wrap(~feature, scales = "fixed") +
coord_flip() +
theme(legend.position = "none")
food_consumption %>%
group_by(food_category) %>%
summarise(consumption = mean(consumption),
co2_emmission = mean(co2_emmission)) %>%
ggplot(aes(x = consumption, y = co2_emmission, color = food_category)) +
geom_point() +
geom_abline(slope = 1) +
scale_x_log10() +
scale_y_log10()
food_consumption %>%
group_by(food_category) %>%
summarise(consumption = mean(consumption),
co2_emmission = mean(co2_emmission)) %>%
mutate(residuals = co2_emmission-consumption) %>%
ggplot(aes(x = consumption, y = co2_emmission, color = food_category)) +
geom_point() +
geom_segment(aes(xend = consumption, yend = consumption, x = consumption, y = co2_emmission)) +
geom_abline(slope = 1) +
scale_x_log10() +
scale_y_log10()
food_consumption %>%
group_by(food_category) %>%
summarise(consumption = mean(consumption),
co2_emmission = mean(co2_emmission)) %>%
mutate(residuals = co2_emmission-consumption) %>%
ggplot(aes(x = consumption, y = co2_emmission, color = food_category)) +
geom_point() +
geom_segment(aes(xend = consumption, yend = consumption, x = consumption, y = co2_emmission)) +
geom_abline(slope = 1) +
scale_x_log10() +
scale_y_log10()
#Top 5 consumers of each food category
food_consumption %>%
group_by(food_category) %>%
top_n(consumption, n = 5) %>%
arrange(food_category, -consumption)
#Coutries that appear more than once in the top 5 consumers by food category
food_consumption %>%
group_by(food_category) %>%
top_n(consumption, n = 5) %>%
arrange(food_category, -consumption) %>%
ungroup() %>%
count(country, sort = TRUE) %>%
filter(n != 1)
#T test between animal and non animal for consumption and co2 emission
library(broom)
food_consumption %>%
mutate(vegan = if_else(food_category %in% c("Wheat and Wheat Products", "Rice", "Soybeans", "Nuts inc. Peanut Butter"), "Non-Animal Product", "Animal Product")) %>%
select(consumption, co2_emmission, vegan) %>%
gather(key = "type", value = "value", -vegan) %>%
mutate(type = as.factor(type),
vegan = as.factor(vegan)) %>%
group_by(type) %>%
do(test = t.test(value~vegan, data = (.))) %>%
tidy(test) %>%
ggplot(aes(x = type, y = p.value)) + geom_col() + geom_hline(yintercept = .05)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
theme_set(theme_light())
# You can use this url to download the data directly into R (will take a few seconds)
restaurant_inspections_raw <- read_csv("https://data.cityofnewyork.us/api/views/43nn-pn8j/rows.csv")
restaurant_inspections <- restaurant_inspections_raw %>%
janitor::clean_names() %>%
select(-phone, -grade_date, -record_date, -building, -street) %>%
mutate(inspection_date = mdy(inspection_date)) %>%
separate(inspection_type, c("inspection_program", "inspection_type"), sep = " / ")
restaurant_inspections %>%
count(dba, camis, sort = TRUE)
restaurant_inspections %>%
count(year = year(inspection_date))
restaurant_inspections %>%
count(grade, sort = TRUE)
restaurant_inspections %>%
count(violation_code, violation_description, sort = TRUE)
restaurant_inspections %>%
filter(camis == 41297769, inspection_date == "2018-09-25") %>%
count(camis, dba, inspection_date, sort = TRUE)
restaurant_inspections %>%
count(cuisine_description, sort = TRUE)
restaurant_inspections %>%
filter(action == "No violations were recorded at the time of this inspection.") %>%
count(critical_flag)
inspections <- restaurant_inspections %>%
group_by(camis,
dba,
boro,
zipcode,
cuisine_description,
inspection_date,
action,
score,
grade,
inspection_type,
inspection_program) %>%
summarize(critical_violations = sum(critical_flag == "Critical", na.rm = TRUE),
non_critical_violations = sum(critical_flag == "Not Critical", na.rm = TRUE)) %>%
ungroup()
most_recent_cycle_inspection <- inspections %>%
filter(inspection_program == "Cycle Inspection",
inspection_type == "Initial Inspection") %>%
arrange(desc(inspection_date)) %>%
distinct(camis, .keep_all = TRUE)
by_dba <- most_recent_cycle_inspection %>%
group_by(dba, cuisine = cuisine_description) %>%
summarize(locations = n(),
avg_score = mean(score),
median_score = median(score)) %>%
ungroup() %>%
arrange(desc(locations))
most_recent_cycle_inspection
inspections <- restaurant_inspections %>%
group_by(camis,
dba,
boro,
zipcode,
cuisine_description,
inspection_date,
action,
score,
grade,
inspection_type,
inspection_program) %>%
summarize(critical_violations = sum(critical_flag == "Critical", na.rm = TRUE),
non_critical_violations = sum(critical_flag == "Not Critical", na.rm = TRUE)) %>%
ungroup()
most_recent_cycle_inspection <- inspections %>%
filter(inspection_program == "Cycle Inspection",
inspection_type == "Initial Inspection") %>%
arrange(desc(inspection_date)) %>%
distinct(camis, .keep_all = TRUE)
most_recent_cycle_inspection
by_dba <- most_recent_cycle_inspection %>%
group_by(dba, cuisine = cuisine_description) %>%
summarize(locations = n(),
avg_score = mean(score),
median_score = median(score)) %>%
ungroup() %>%
arrange(desc(locations))
by_cuisine <- by_dba %>%
group_by(cuisine) %>%
summarize(avg_score = mean(avg_score),
median_score = median(avg_score),
restaurants = n()) %>%
arrange(desc(restaurants))
library(broom)
cuisine_conf_ints <- by_dba %>%
add_count(cuisine) %>%
filter(n > 100) %>%
nest(-cuisine) %>%
mutate(model = map(data, ~ t.test(.$avg_score))) %>%
unnest(map(model, tidy))
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
attendance <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-04/attendance.csv')
standings <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-04/standings.csv')
games <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-04/games.csv')
attendance %>% glimpse()
standings %>% glimpse()
games %>% glimpse()
#What are the distributions of game metrics by week?
#Are there any seasonal trends?
games %>%
select(week, (10:15)) %>%
filter(!week %in% c("WildCard", "SuperBowl", "Division","ConfChamp")) %>%
gather(key = "key", value = "value", -week) %>%
mutate(week = reorder(week, week %>% as.numeric())) %>%
ggplot(aes(x = week, y= value, color = key)) +
geom_boxplot() +
facet_wrap(~key, scales = "free") +
theme(legend.position = "none")
#How do teams perform when they're playing at home vs playing awawy?
games %>%
select(home = home_team_name, away = away_team_name, (10:15)) %>%
gather(key = "key", value = "value", -home, -away) %>%
gather(key = "homeaway", value = "team", -key, -value) %>%
select(team, homeaway, key, value) %>%
group_by(homeaway, team, key) %>%
summarise(mean = mean(value)) %>%
pivot_wider(names_from = "homeaway", values_from = "mean") %>%
mutate(margin = home-away) %>%
gather(key = "homeaway", value = "value", -key, -margin, -team) %>%
ggplot(aes(x = reorder(team, -margin), y = value, fill = homeaway)) +
geom_col(color = "white", position = "dodge") +
facet_wrap(~key, scales = "free") +
coord_flip() +
theme(legend.position = "top")
#Better visual for looking at how teams perform home and away for all of the metrics
games %>%
select(home = home_team_name, away = away_team_name, (10:15)) %>%
gather(key = "key", value = "value", -home, -away) %>%
gather(key = "homeaway", value = "team", -key, -value) %>%
select(team, homeaway, key, value) %>%
group_by(homeaway, team, key) %>%
summarise(mean = mean(value)) %>%
pivot_wider(names_from = "homeaway", values_from = "mean") %>%
mutate(margin = home-away) %>%
select(team, key, margin) %>%
mutate(advantage = if_else(margin > 0, "Better at Home", "Better at Away")) %>%
ggplot(aes(x = reorder(team, margin), y = margin, color = advantage)) +
geom_point() +
geom_segment(aes(xend = team, yend = 0)) +
facet_wrap(~key, scales = "free") +
theme(legend.position = "top") +
xlab("") +
ylab("") +
coord_flip()
games %>%
select(home = home_team_name, away = away_team_name, (10:15)) %>%
gather(key = "key", value = "value", -home, -away) %>%
gather(key = "homeaway", value = "team", -key, -value) %>%
select(team, homeaway, key, value) %>%
group_by(homeaway, team, key) %>%
summarise(mean = mean(value)) %>%
pivot_wider(names_from = "homeaway", values_from = "mean") %>%
mutate(margin = home-away) %>%
select(team, key, margin) %>%
mutate(advantage = if_else(margin > 0, "Better at Home", "Better at Away"))
games %>%
select(home = home_team_name, away = away_team_name, (10:15)) %>%
gather(key = "key", value = "value", -home, -away) %>%
gather(key = "homeaway", value = "team", -key, -value) %>%
select(team, homeaway, key, value) %>%
group_by(team, key) %>%
do(ttest = tidy(t.test(value~homeaway, data = (.)))) %>%
unnest(ttest) %>%
select(team, key, p.value) %>%
ggplot(aes(x = key, y = p.value, color = team)) +
geom_point() +
geom_segment(aes(xend = key, yend = 0)) +
geom_hline(yintercept = .05, linetype = "dashed", color = "red") +
facet_wrap(~team, scales = "free") +
theme(legend.position = "none") +
coord_flip()
shiny::runApp('~/Desktop/agenus_app_v9/rep_input_ex')
runApp('~/Desktop/agenus_app_v9/rep_input_ex')
runApp('~/Desktop/agenus_app_v9/rep_input_ex')
library(knitr)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE,
echo = TRUE, dpi = 300, cache.lazy = FALSE,
tidy = "styler", fig.width = 10, fig.height = 6,
fig.path = "static")
options(cli.width = 70, width = 70)
library(tidyverse)
library(tidymodels)
library(scales)
library(lubridate)
library(tidytext)
theme_set(theme_light())
# Load the data with a readr function
crop_yields <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-01/key_crop_yields.csv") %>%
janitor::clean_names()
land_use <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-01/land_use_vs_yield_change_in_cereal_production.csv") %>%
janitor::clean_names() %>%
select(entity, code, year, population = total_population_gapminder) %>%
mutate(year = as.numeric(year))
crop_yield <- crop_yields %>%
left_join(land_use, by = c('code',  'year', 'entity'))
skimr::skim(crop_yields)
# Here are a few other options for getting to know the data
# glimpse(crop_yields)
# summary(crop_yields)
# View(crop_yields)
?slice_sample
# Load the data with a readr function
crop_yields <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-01/key_crop_yields.csv") %>%
janitor::clean_names()
land_use <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-01/land_use_vs_yield_change_in_cereal_production.csv") %>%
janitor::clean_names() %>%
select(entity, code, year, population = total_population_gapminder) %>%
mutate(year = as.numeric(year))
crop_yield <- crop_yields %>%
left_join(land_use, by = c('code',  'year', 'entity'))
skimr::skim(crop_yields)
# Here are a few other options for getting to know the data
# glimpse(crop_yields)
# summary(crop_yields)
# View(crop_yields)
# Use pivot_longer from tidyr to tidy the data.
crop_yield_tidy <- crop_yield %>%
rename_all(str_remove, "_tonnes.*") %>%
rename(country = entity) %>%
pivot_longer(wheat:bananas, names_to = 'crop', values_to = 'yield_hectare') %>%
drop_na(yield_hectare)
# Create a vector of the 25 countries with the largest population
top_pops <- land_use  %>%
filter(!is.na(code), entity != "World") %>%
group_by(entity) %>%
filter(year == max(year)) %>%
ungroup() %>%
slice_max(population, n = 25) %>%
pull(entity)
kable(head(crop_yield_tidy, n = 10))
crop_yield_tidy %>%
add_count(year, country, sort = T) %>%
filter(country %in% c("Albania", "Africa", "United States")) # Africa has 11 crops, US has 9 and Albania 6.
crop_yield_tidy %>%
count(country, sort = TRUE) %>%
filter(n >=500) # this gives a list of the countries with the most years + crops 11 (crops) * 51 (year) = 561
crop_yield_tidy %>%
summarize(max(year)- min(year)) #57 years of recorded data
crop_yield_tidy %>% # 249 unique countries
distinct(country) %>%
nrow()
crop_yield_tidy %>%
filter(country == 'United States') %>%
ggplot(aes(year, yield_hectare, color = crop)) +
geom_line() +
facet_wrap(~country, 'free_x') +
theme(plot.title.position = 'plot') +
labs(
title = 'Crop Yield USA',
y = 'Year',
x = 'tonnes per hectare per year'
)
# plot of USA data only
crop_yield_tidy %>%
filter(country == 'United States') %>%
mutate(crop = fct_reorder(crop, yield_hectare)) %>%
ggplot(aes(yield_hectare, crop, fill = crop)) +
geom_col()  +
facet_wrap(~country, scales = 'free_y') +
theme(legend.position =  'none',
plot.title.position = 'plot') +
labs(
title = 'Crop Yield USA',
y = 'Crop',
x = 'tonnes per hectare'
)
plotly::ggplotly(us_yields)
us_yields <- crop_yield_tidy %>%
filter(country == 'United States') %>%
ggplot(aes(year, yield_hectare, color = crop)) +
geom_line() +
facet_wrap(~country, 'free_x') +
theme(plot.title.position = 'plot') +
labs(
title = 'Crop Yield USA',
y = 'Year',
x = 'tonnes per hectare per year'
)
plotly::ggplotly(us_yields)
# set.seed(1014) if you want to select the same 9 countries each time
# Randomly select 9 country names
crops <- crop_yield_tidy %>% distinct(crop) %>%  pull()
country_random <- crop_yield_tidy %>%
select(country) %>%
distinct() %>%
slice_sample(n = 9) %>%
pull()
# Plot crop yield per hectare per year.
crop_yield_tidy %>%
filter(country %in% country_random,
crop %in% c('maize', 'potatoes', 'wheat', 'bananas')) %>%
mutate(crop = fct_reorder(crop, yield_hectare)) %>%
ggplot(aes(year, yield_hectare, color = crop)) +
geom_line(size = 1, alpha = 0.5) +
facet_wrap(~country, scales = 'free_y') +
theme(plot.title.position = 'plot') +
labs(
y = 'yield per hectare',
title = 'Crop Yield'
)
crop_yield_tidy %>%
filter(country %in% top_pops,
crop == c('maize', 'potatoes', 'wheat', 'bananas')) %>%
mutate(crop = reorder_within(crop, yield_hectare, country)) %>%
ggplot(aes(yield_hectare, crop, fill = crop)) +
geom_col()  +
scale_y_reordered() +
facet_wrap(~country, scales = 'free') +
theme(legend.position = 'none',
plot.title.position = 'plot') +
labs(
title = 'Crop Yield',
y = 'Crop',
x = 'yield per hectare'
)
crop_yield_tidy %>%
filter(country %in% top_pops,
crop %in% c('maize', 'potatoes', 'wheat', 'bananas')) %>%
mutate(crop = fct_reorder(crop, yield_hectare)) %>%
ggplot(aes(year, yield_hectare, color = crop)) +
geom_line(size = 1, alpha = 0.5) +
theme(plot.title.position = 'plot') +
facet_wrap(~country, scales = 'free_y') +
labs(
y = 'yield per hectare',
title = 'Crop Yield'
)
many_models <- crop_yield_tidy %>%
drop_na() %>%
filter(country %in% top_pops,
crop %in% c('maize', 'potatoes', 'wheat', 'bananas')) %>%
nest(mod_data = c(year, yield_hectare, population)) %>%
mutate(model = map(mod_data, ~lm(yield_hectare ~ population + year, data = .))) %>%
mutate(coefs = map(model, tidy)) %>%
unnest(coefs)
many_models
many_models %>%
filter(term == "year") %>%
mutate(p.value = p.adjust(p.value)) %>%
ggplot(aes(estimate, p.value, label = country)) +
geom_vline(xintercept = 0, lty = 2,
size = 1.5, alpha = 0.7, color = "gray50") +
geom_point(aes(color = crop), alpha = 0.8, size = 2.5, show.legend = FALSE) +
scale_y_log10() +
facet_wrap(~crop) +
geom_text_repel(size = 2.5, family = "Times New Roman")
library(ggrepel)
many_models %>%
filter(term == "year") %>%
mutate(p.value = p.adjust(p.value)) %>%
ggplot(aes(estimate, p.value, label = country)) +
geom_vline(xintercept = 0, lty = 2,
size = 1.5, alpha = 0.7, color = "gray50") +
geom_point(aes(color = crop), alpha = 0.8, size = 2.5, show.legend = FALSE) +
scale_y_log10() +
facet_wrap(~crop) +
geom_text_repel(size = 2.5, family = "Times New Roman")
blogdown::serve_site()
blogdown::serve_site()
blogdown:::preview_site()
blogdown::serve_site()
options(blogdown.server.timeout = 600)
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::build_site()
blogdown::serve_site()
blogdown::serve_site()
?serve_site
?blogdown::serve_site
?blogdown::serve_site
options(blogdown.server.timeout = 600)
blogdown::serve_site()
getwd()
ls()
blogdown::serve_site()
list.files('content', '.Rmd$', full.names = TRUE, recursive = TRUE)
blogdown::check_config()
blogdown::serve_site()
devtools::session_info('blogdown')
library(devtools)
devtools::session_info('blogdown')
blogdown::serve_site()
blogdown::update_hugo()
blogdown::install_hugo()
blogdown::serve_site(
)
blogdown::hugo_cmd('server')
remotes::install_github('rstudio/blogdown')
blogdown::serve_site()
blogdown::serve_site()
hugo server -D
hugo server -D
hugo server -D
